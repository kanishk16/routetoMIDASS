{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python_Problem.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanishk16/routetoMIDASS/blob/master/Python_Problem/Python_Problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "feTUySxqcVmw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">**Necessary Installations & Imports**"
      ]
    },
    {
      "metadata": {
        "id": "S8n8DD1kvsAl",
        "colab_type": "code",
        "outputId": "c536ef82-2922-4766-af35-2c5b376a2f79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install jsonlines"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JsgcVXdHuK39",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import json\n",
        "import jsonlines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jlpZdnmS5Nkw",
        "colab_type": "code",
        "outputId": "9660d2e8-3acc-4d14-d12e-980b09b30ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!wget -c https://github.com/wbolster/jsonlines/tree/1.2.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-06 18:58:10--  https://github.com/wbolster/jsonlines/tree/1.2.0\n",
            "Resolving github.com (github.com)... 192.30.255.112, 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘1.2.0’\n",
            "\n",
            "\r1.2.0                   [<=>                 ]       0  --.-KB/s               \r1.2.0                   [ <=>                ]  82.30K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2019-04-06 18:58:10 (5.48 MB/s) - ‘1.2.0’ saved [84279]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C4zgnHyLc2kU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> **Task 1a : A python script to fetch all the tweets(as many as allowed by Twitter API) done by @midasIIITD twitter handle and dump the responses into JSONlines file**"
      ]
    },
    {
      "metadata": {
        "id": "yMZAAqsVdUPP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> **Authorization for Twitter API**"
      ]
    },
    {
      "metadata": {
        "id": "2c8KVhnFko67",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Stores tokens and keys\n",
        "\"\"\"\n",
        "consumer_key = \"f69Ch2S4kVRsXbaZwhs0Qhc7M\"\n",
        "consumer_secret = \"Z43Y85EwvKe3tOHVNAfvxEx7r7jmNliHhXDgZsdSnXuRIZKLKz\"\n",
        "access_key = \"1034447027923374082-ftqEyERgMoJ8s8sMFICXfxy7UC44vs\"\n",
        "access_secret = \"p8hg6CRuXVzn9yFxrnwObSCrm0gmW2PhKiveqw9SLwzsg\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AMc6oh8rgXtI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Authorization to consumer key and consumer secret \n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
        "\n",
        "# Access to user's access key and access secret \n",
        "auth.set_access_token(access_key, access_secret) \n",
        "\n",
        "# Calling api \n",
        "api = tweepy.API(auth) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RPzLPwt7gwyu",
        "colab_type": "code",
        "outputId": "10e437b1-68f9-4ed9-98f7-d1b801812bc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# api check\n",
        "api"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tweepy.api.API at 0x7fb4d6753d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "e7a82i6OgfCr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">**Tweet Extraction**"
      ]
    },
    {
      "metadata": {
        "id": "nBqHA2fjg8tY",
        "colab_type": "code",
        "outputId": "f2d6e90f-522c-4d55-c6ad-5d8507abf487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# tweets to be extracted \n",
        "tweets = api.user_timeline(screen_name='midasIIITD', number_of_tweets=10) \n",
        "\n",
        "# tweet check\n",
        "print(type(tweets))\n",
        "print(type(tweets[0]))\n",
        "print(type(tweets[0]._json))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tweepy.models.ResultSet'>\n",
            "<class 'tweepy.models.Status'>\n",
            "<class 'dict'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mVEFKUY_hUU-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> ** Tweepy Status Object to dict to JSON string (ENCODING) to dict to list(DECODING)**\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0I6y0Ww2h4z0",
        "colab_type": "code",
        "outputId": "754fc16f-0d2b-46cb-c604-5072ba8c6a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# tweepy status object to json object\n",
        "tweet_x = [json.loads(json.dumps(tweet._json)) for tweet in tweets]\n",
        "\n",
        "# tweet_x check\n",
        "print(type(tweet_x))\n",
        "print(type(tweet_x[1]))\n",
        "print(type(json.dumps(tweets[0]._json)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'dict'>\n",
            "<class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ElzxdMvOiWov",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">** dump into a jsonlines file**"
      ]
    },
    {
      "metadata": {
        "id": "vf9Bp57zkWLO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Open a JSON file to dump all the tweets\n",
        "with open('/content/twitter12.jsonl', mode='w') as writer:\n",
        "  for ii in tweet_x:\n",
        "    json.dump(ii, writer)\n",
        "    writer.write('\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XsXe9I41d_lt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> **Function to fetch all tweets and dump their responses into a JSONL file**"
      ]
    },
    {
      "metadata": {
        "id": "HubLxZVqlBJs",
        "colab_type": "code",
        "outputId": "303c1d94-592d-405b-fe65-e109eb2b9766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Function to fetch all tweets and dump them into a jsonlines file \n",
        "def get_tweets(username): \n",
        "          \n",
        "        # Authorization to consumer key and consumer secret \n",
        "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
        "  \n",
        "        # Access to user's access key and access secret \n",
        "        auth.set_access_token(access_key, access_secret) \n",
        "  \n",
        "        # Calling api \n",
        "        api = tweepy.API(auth) \n",
        "  \n",
        "        # tweets to be extracted \n",
        "        tweets = api.user_timeline(screen_name=username, count=200)\n",
        "        \n",
        "        # tweepy status object to json object\n",
        "        tweet_x = [json.loads(json.dumps(tweet._json)) for tweet in tweets] \n",
        "        \n",
        "       # Open a JSON file to dump all the tweets\n",
        "        with open('/content/twitter.jsonl', mode='w') as writer:\n",
        "          for ii in tweet_x:\n",
        "            json.dump(ii, writer)\n",
        "            writer.write('\\n')\n",
        "            \n",
        "        # Printing the tweets \n",
        "        print(\"RECORDED\") \n",
        "  \n",
        "  \n",
        "if __name__ == '__main__': \n",
        "  \n",
        "    # Twitter handle whose tweets are to be extracted \n",
        "    get_tweets(\"midasIIITD\") "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RECORDED\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7igwJn1TXnqm",
        "colab_type": "code",
        "outputId": "c25d5051-4a25-40a4-9da3-7961ee6ea974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.22.0)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.14.6)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aoKNEKQEcx04",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Task1b: Parse the tweets from the .jsonl file and convert them into a tabular format \n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_DPinGrlX057",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#tweet_store = []\n",
        "with jsonlines.open('/content/twitter.jsonl', mode='r') as reader:\n",
        "  for r in reader:\n",
        "    tweet_store = [json.loads(json.dumps(r)) for r in reader]\n",
        "    \n",
        "    # sanity check \n",
        "    print(type(tweet_store))\n",
        "    print(type(tweet_store[0]))\n",
        "\n",
        "df = pd.DataFrame(tweet_store, columns=['text','created_at','favorite_count','retweet_count'])    \n",
        " \n",
        "print(df) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BTv95rAjd6to",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "images = []\n",
        "for tweet in tweet_store:\n",
        "  try:\n",
        "    if tweet['entities']['media'][0]['type'] == 'photo':\n",
        "      image_count += 1\n",
        "  except:\n",
        "    image_count = 0\n",
        "  images.append(image_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kgK-fR7Gig4B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3394
        },
        "outputId": "8b50dded-010c-4e54-994b-ecbc7a153272"
      },
      "cell_type": "code",
      "source": [
        "df['images_count'] = images\n",
        "\n",
        "print(df)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  text  \\\n",
            "0    RT @IIITDelhi: Applications open for MTech (CB...   \n",
            "1    RT @IIITDelhi: We are delighted to share that ...   \n",
            "2    RT @Harvard: Professor Jelani Nelson founded A...   \n",
            "3    RT @emnlp2019: For anyone interested in submit...   \n",
            "4    RT @multimediaeval: Announcing the 2019 MediaE...   \n",
            "5    Many Congratulations to @midasIIITD student, S...   \n",
            "6    @midasIIITD thanks all students who have appea...   \n",
            "7    @himanchalchandr Meanwhile, complete CV/NLP ta...   \n",
            "8    @sayangdipto123 Submit as per the guideline ag...   \n",
            "9    We request all students whose interview are sc...   \n",
            "10   Other queries: \"none of the Tweeter Apis give ...   \n",
            "11   Other queries: \"do we have to make two differe...   \n",
            "12   Other queries: \"If using Twitter api, it does ...   \n",
            "13   Response to some queries asked by students on ...   \n",
            "14   RT @kdnuggets: Top 8 #Free Must-Read #Books on...   \n",
            "15   @nupur_baghel @PennDATS Congratulation @nupur_...   \n",
            "16   We have emailed the task details to all candid...   \n",
            "17   RT @rfpvjr: Our NAACL paper on polarization in...   \n",
            "18   RT @kdnuggets: Effective Transfer Learning For...   \n",
            "19   RT @stanfordnlp: What’s new in @Stanford CS224...   \n",
            "20   RT @DeepMindAI: Today we're releasing a large-...   \n",
            "21   RT @ylecun: Congratulations Jitendra Malik !\\n...   \n",
            "22   RT @IIITDelhi: Another chance to take admissio...   \n",
            "23   Dear @midasIIITD internship candidates who hav...   \n",
            "24   Looking forward to your paper submission to @I...   \n",
            "25   RT @ngrams: Reproducibility in multimedia rese...   \n",
            "26   Online application for https://t.co/DJFDrQsHZP...   \n",
            "27   RT @ACMMM19: A final reminder of the Reproduci...   \n",
            "28   RT @isarth23: Thanks for the support and help ...   \n",
            "29   Since SemEval-2019 will be held June 6-7, 2019...   \n",
            "..                                                 ...   \n",
            "169  RT @RatnRajiv: Fortunate to have appreciation ...   \n",
            "170  RT @Harvard: The study provides potential futu...   \n",
            "171  RT @UniofOxford: In the final episode of serie...   \n",
            "172  @ACMMM19: Call for Workshop Proposals \\n\\nDead...   \n",
            "173  @ACMMM19: Call for Grand Challenge Proposals\\n...   \n",
            "174  Since 1993, ACM Multimedia has been bringing t...   \n",
            "175  @ACMMM19 is structured around four themes (alp...   \n",
            "176  ACM Multimedia 2019: First Call for Papers/Cal...   \n",
            "177  RT @ACMMM19: Happy new year! 2019 started with...   \n",
            "178  You may follow @midasIIITD updates through fol...   \n",
            "179  Today, @midasIIITD completed one year at @IIIT...   \n",
            "180  RT @debanjanbhucs: Happy anniversary to @midas...   \n",
            "181  Two Research Assistant (RA) positions are stil...   \n",
            "182  Positions for two Research Assistants (RA) are...   \n",
            "183  RT @TCoolsIT: I wrote a blogpost on CoreNLP hi...   \n",
            "184  @RatnRajiv delivered a research talk at the Ce...   \n",
            "185  @midasIIITD look forward to work together. htt...   \n",
            "186  @the_dhumketu @punnibhai @Hitkul_, FYI. https:...   \n",
            "187  RT @iiit_hyderabad: KCIS Distinguished Lecture...   \n",
            "188  RT @StanfordAILab: Our latest blog post is out...   \n",
            "189  RT @medialab: In two new papers, @PratikShahPh...   \n",
            "190  RT @RatnRajiv: Wonderful get together with our...   \n",
            "191  @ACMMM19 is the premier international conferen...   \n",
            "192  Feel free to contact us if you have any query ...   \n",
            "193  @NilayShri @RatnRajiv @ACMMM19 Looking forward...   \n",
            "194  @midasIIITD head, Dr. @RatnRajiv appointed as ...   \n",
            "195  @ACMMM19 @the_dhumketu fill the volunteer form...   \n",
            "196  RT @ACMMM19: New in 2019! Volunteer to serve a...   \n",
            "197  RT @IIITDelhi: Congratulations! authors Yaman ...   \n",
            "198  Best wishes to @midasIIITD students @_himanshu...   \n",
            "\n",
            "                         created_at  favorite_count  retweet_count  \\\n",
            "0    Wed Apr 10 04:51:26 +0000 2019               0              1   \n",
            "1    Tue Apr 09 16:45:07 +0000 2019               0             14   \n",
            "2    Tue Apr 09 05:04:27 +0000 2019               0             35   \n",
            "3    Tue Apr 09 05:04:11 +0000 2019               0             17   \n",
            "4    Mon Apr 08 19:38:09 +0000 2019               0             15   \n",
            "5    Mon Apr 08 07:08:12 +0000 2019              18              2   \n",
            "6    Mon Apr 08 03:27:42 +0000 2019               5              0   \n",
            "7    Sun Apr 07 14:17:29 +0000 2019               0              0   \n",
            "8    Sun Apr 07 14:17:09 +0000 2019               0              0   \n",
            "9    Sun Apr 07 11:43:24 +0000 2019               1              1   \n",
            "10   Sun Apr 07 06:55:19 +0000 2019               5              2   \n",
            "11   Sun Apr 07 06:53:38 +0000 2019               4              1   \n",
            "12   Sun Apr 07 05:32:27 +0000 2019               6              1   \n",
            "13   Sun Apr 07 05:29:40 +0000 2019               7              1   \n",
            "14   Sat Apr 06 17:11:29 +0000 2019               0              2   \n",
            "15   Sat Apr 06 16:43:27 +0000 2019              18              3   \n",
            "16   Fri Apr 05 16:08:37 +0000 2019              11              1   \n",
            "17   Fri Apr 05 04:05:11 +0000 2019               0             16   \n",
            "18   Fri Apr 05 04:04:43 +0000 2019               0             11   \n",
            "19   Wed Apr 03 18:31:53 +0000 2019               0             59   \n",
            "20   Wed Apr 03 17:04:32 +0000 2019               0            872   \n",
            "21   Wed Apr 03 09:03:40 +0000 2019               0             16   \n",
            "22   Wed Apr 03 07:46:02 +0000 2019               0              4   \n",
            "23   Tue Apr 02 04:20:13 +0000 2019               8              1   \n",
            "24   Tue Apr 02 02:44:54 +0000 2019               5              1   \n",
            "25   Tue Apr 02 02:35:44 +0000 2019               0              7   \n",
            "26   Mon Apr 01 06:53:08 +0000 2019               7              2   \n",
            "27   Sun Mar 31 10:21:24 +0000 2019               0             10   \n",
            "28   Fri Mar 29 19:43:24 +0000 2019               0              2   \n",
            "29   Fri Mar 29 17:16:40 +0000 2019               9              1   \n",
            "..                              ...             ...            ...   \n",
            "169  Thu Jan 10 08:48:17 +0000 2019               0              3   \n",
            "170  Wed Jan 09 02:05:06 +0000 2019               0              9   \n",
            "171  Mon Jan 07 09:05:12 +0000 2019               0             16   \n",
            "172  Sun Jan 06 12:54:06 +0000 2019               1              0   \n",
            "173  Sun Jan 06 04:29:25 +0000 2019               1              0   \n",
            "174  Sun Jan 06 04:09:54 +0000 2019               5              1   \n",
            "175  Sun Jan 06 04:07:29 +0000 2019               1              0   \n",
            "176  Sun Jan 06 04:05:38 +0000 2019               4              2   \n",
            "177  Thu Jan 03 06:14:05 +0000 2019               0              2   \n",
            "178  Tue Jan 01 17:21:49 +0000 2019               4              2   \n",
            "179  Tue Jan 01 17:17:56 +0000 2019              14              5   \n",
            "180  Tue Jan 01 16:14:24 +0000 2019               0              4   \n",
            "181  Thu Dec 27 10:28:31 +0000 2018               2              1   \n",
            "182  Thu Dec 27 09:59:06 +0000 2018               5              3   \n",
            "183  Thu Dec 27 02:33:58 +0000 2018               0             12   \n",
            "184  Wed Dec 26 09:41:04 +0000 2018              14              2   \n",
            "185  Tue Dec 25 04:15:15 +0000 2018               1              0   \n",
            "186  Tue Dec 25 03:00:21 +0000 2018               2              1   \n",
            "187  Mon Dec 24 07:39:21 +0000 2018               0             11   \n",
            "188  Fri Dec 21 03:50:39 +0000 2018               0             32   \n",
            "189  Thu Dec 20 18:05:35 +0000 2018               0              8   \n",
            "190  Thu Dec 20 06:14:52 +0000 2018               0              3   \n",
            "191  Wed Dec 19 02:16:31 +0000 2018               2              3   \n",
            "192  Tue Dec 18 14:42:27 +0000 2018               3              2   \n",
            "193  Tue Dec 18 14:27:12 +0000 2018               3              0   \n",
            "194  Tue Dec 18 14:15:32 +0000 2018              12              5   \n",
            "195  Tue Dec 18 12:34:10 +0000 2018               1              0   \n",
            "196  Tue Dec 18 12:33:42 +0000 2018               0              7   \n",
            "197  Tue Dec 18 04:38:35 +0000 2018               0              3   \n",
            "198  Mon Dec 17 16:57:00 +0000 2018              14              4   \n",
            "\n",
            "     images_count  \n",
            "0               0  \n",
            "1               0  \n",
            "2               0  \n",
            "3               0  \n",
            "4               0  \n",
            "5               0  \n",
            "6               0  \n",
            "7               0  \n",
            "8               0  \n",
            "9               0  \n",
            "10              0  \n",
            "11              0  \n",
            "12              0  \n",
            "13              0  \n",
            "14              0  \n",
            "15              0  \n",
            "16              0  \n",
            "17              0  \n",
            "18              1  \n",
            "19              0  \n",
            "20              0  \n",
            "21              0  \n",
            "22              0  \n",
            "23              0  \n",
            "24              0  \n",
            "25              0  \n",
            "26              0  \n",
            "27              0  \n",
            "28              0  \n",
            "29              0  \n",
            "..            ...  \n",
            "169             0  \n",
            "170             0  \n",
            "171             0  \n",
            "172             0  \n",
            "173             0  \n",
            "174             0  \n",
            "175             0  \n",
            "176             0  \n",
            "177             0  \n",
            "178             0  \n",
            "179             0  \n",
            "180             0  \n",
            "181             0  \n",
            "182             0  \n",
            "183             0  \n",
            "184             0  \n",
            "185             0  \n",
            "186             0  \n",
            "187             0  \n",
            "188             0  \n",
            "189             0  \n",
            "190             0  \n",
            "191             0  \n",
            "192             0  \n",
            "193             0  \n",
            "194             0  \n",
            "195             0  \n",
            "196             0  \n",
            "197             0  \n",
            "198             0  \n",
            "\n",
            "[199 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}