{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python_Problem.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanishk16/routetoMIDASS/blob/master/Python_Problem/Python_Problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "feTUySxqcVmw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">**Necessary Installations & Imports**"
      ]
    },
    {
      "metadata": {
        "id": "S8n8DD1kvsAl",
        "colab_type": "code",
        "outputId": "6908bb5f-d8db-4c1b-e2b6-bf9d5435a0db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install jsonlines"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JsgcVXdHuK39",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import json\n",
        "import jsonlines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jlpZdnmS5Nkw",
        "colab_type": "code",
        "outputId": "9660d2e8-3acc-4d14-d12e-980b09b30ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!wget -c https://github.com/wbolster/jsonlines/tree/1.2.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-06 18:58:10--  https://github.com/wbolster/jsonlines/tree/1.2.0\n",
            "Resolving github.com (github.com)... 192.30.255.112, 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘1.2.0’\n",
            "\n",
            "\r1.2.0                   [<=>                 ]       0  --.-KB/s               \r1.2.0                   [ <=>                ]  82.30K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2019-04-06 18:58:10 (5.48 MB/s) - ‘1.2.0’ saved [84279]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C4zgnHyLc2kU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> **Task 1a : A python script to fetch all the tweets(as many as allowed by Twitter API) done by @midasIIITD twitter handle and dump the responses into JSONlines file**"
      ]
    },
    {
      "metadata": {
        "id": "yMZAAqsVdUPP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> **Authorization for Twitter API**"
      ]
    },
    {
      "metadata": {
        "id": "2c8KVhnFko67",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Stores tokens and keys\n",
        "\"\"\"\n",
        "consumer_key = \"f69Ch2S4kVRsXbaZwhs0Qhc7M\"\n",
        "consumer_secret = \"Z43Y85EwvKe3tOHVNAfvxEx7r7jmNliHhXDgZsdSnXuRIZKLKz\"\n",
        "access_key = \"1034447027923374082-ftqEyERgMoJ8s8sMFICXfxy7UC44vs\"\n",
        "access_secret = \"p8hg6CRuXVzn9yFxrnwObSCrm0gmW2PhKiveqw9SLwzsg\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AMc6oh8rgXtI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Authorization to consumer key and consumer secret \n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
        "\n",
        "# Access to user's access key and access secret \n",
        "auth.set_access_token(access_key, access_secret) \n",
        "\n",
        "# Calling api \n",
        "api = tweepy.API(auth) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RPzLPwt7gwyu",
        "colab_type": "code",
        "outputId": "10e437b1-68f9-4ed9-98f7-d1b801812bc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# api check\n",
        "api"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tweepy.api.API at 0x7fb4d6753d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "e7a82i6OgfCr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">**Tweet Extraction**"
      ]
    },
    {
      "metadata": {
        "id": "nBqHA2fjg8tY",
        "colab_type": "code",
        "outputId": "529b5db9-a106-4ba2-fdcf-5dac754197f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# tweets to be extracted \n",
        "tweets = api.user_timeline(screen_name='midasIIITD', number_of_tweets=10) \n",
        "\n",
        "# tweet check\n",
        "print(type(tweets))\n",
        "print(type(tweets[0]))\n",
        "print(type(tweets[0]._json))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tweepy.models.ResultSet'>\n",
            "<class 'tweepy.models.Status'>\n",
            "<class 'dict'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mVEFKUY_hUU-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> ** Tweepy Status Object to dict to JSON string (ENCODING) to dict to list(DECODING)**\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0I6y0Ww2h4z0",
        "colab_type": "code",
        "outputId": "754fc16f-0d2b-46cb-c604-5072ba8c6a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# tweepy status object to json object\n",
        "tweet_x = [json.loads(json.dumps(tweet._json)) for tweet in tweets]\n",
        "\n",
        "# tweet_x check\n",
        "print(type(tweet_x))\n",
        "print(type(tweet_x[1]))\n",
        "print(type(json.dumps(tweets[0]._json)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'dict'>\n",
            "<class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ElzxdMvOiWov",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">** dump into a jsonlines file**"
      ]
    },
    {
      "metadata": {
        "id": "vf9Bp57zkWLO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Open a JSON file to dump all the tweets\n",
        "with open('/content/twitter12.jsonl', mode='w') as writer:\n",
        "  for ii in tweet_x:\n",
        "    json.dump(ii, writer)\n",
        "    writer.write('\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XsXe9I41d_lt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> **Function to fetch all tweets and dump their responses into a JSONL file**"
      ]
    },
    {
      "metadata": {
        "id": "HubLxZVqlBJs",
        "colab_type": "code",
        "outputId": "a2546fbf-ae5b-4321-8e56-ef6a881d6c5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Function to fetch all tweets and dump them into a jsonlines file \n",
        "def get_tweets(username): \n",
        "          \n",
        "        # Authorization to consumer key and consumer secret \n",
        "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
        "  \n",
        "        # Access to user's access key and access secret \n",
        "        auth.set_access_token(access_key, access_secret) \n",
        "  \n",
        "        # Calling api \n",
        "        api = tweepy.API(auth) \n",
        "  \n",
        "        # tweets to be extracted \n",
        "        tweets = api.user_timeline(screen_name=username, count=200, tweet_mode='extended')\n",
        "        \n",
        "        # tweepy status object to json object\n",
        "        tweet_x = [json.loads(json.dumps(tweet._json)) for tweet in tweets] \n",
        "        \n",
        "       # Open a JSON file to dump all the tweets\n",
        "        with open('/content/twitter.jsonl', mode='w') as writer:\n",
        "          for ii in tweet_x:\n",
        "            json.dump(ii, writer)\n",
        "            writer.write('\\n')\n",
        "            \n",
        "        # Printing the tweets \n",
        "        print(\"RECORDED\") \n",
        "  \n",
        "  \n",
        "if __name__ == '__main__': \n",
        "  \n",
        "    # Twitter handle whose tweets are to be extracted \n",
        "    get_tweets(\"midasIIITD\") "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RECORDED\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aoKNEKQEcx04",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Task1b: Parse the tweets from the .jsonl file and convert them into a tabular format \n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "7ec69938-55ae-4d30-be1d-29132ba6cfe2",
        "id": "_DPinGrlX057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#tweet_store = []\n",
        "with jsonlines.open('/content/twitter00.jsonl', mode='r') as reader:\n",
        "  for r in reader:\n",
        "    tweet_store = [json.loads(json.dumps(r)) for r in reader]\n",
        "    \n",
        "    # sanity check \n",
        "    print(type(tweet_store))\n",
        "    print(type(tweet_store[0]))\n",
        "\n",
        "df = pd.DataFrame(tweet_store, columns=['full_text','created_at','favorite_count','retweet_count'])    \n",
        " \n",
        "print(df.head()) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'dict'>\n",
            "                                           full_text  \\\n",
            "0  RT @IIITDelhi: Applications open for MTech (CB...   \n",
            "1  RT @IIITDelhi: We are delighted to share that ...   \n",
            "2  RT @Harvard: Professor Jelani Nelson founded A...   \n",
            "3  RT @emnlp2019: For anyone interested in submit...   \n",
            "4  RT @multimediaeval: Announcing the 2019 MediaE...   \n",
            "\n",
            "                       created_at  favorite_count  retweet_count  \n",
            "0  Wed Apr 10 04:51:26 +0000 2019               0              1  \n",
            "1  Tue Apr 09 16:45:07 +0000 2019               0             14  \n",
            "2  Tue Apr 09 05:04:27 +0000 2019               0             35  \n",
            "3  Tue Apr 09 05:04:11 +0000 2019               0             17  \n",
            "4  Mon Apr 08 19:38:09 +0000 2019               0             15  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l6t7KCH9Jbx0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">** Count and store the image_count in a list**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BTv95rAjd6to",
        "outputId": "c3138a04-e772-4d64-a68d-d0e1ac72b0d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "images = []\n",
        "for tweet in tweet_store:\n",
        "  image_count = 0\n",
        "  try:\n",
        "    if tweet['extended_entities']['media'][0]['type'] == 'photo':\n",
        "      image_count += 1\n",
        "  except:\n",
        "    image_count = None\n",
        "  images.append(image_count)\n",
        "\n",
        "# sanity check\n",
        "print(set(images))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1, None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7tT4b1YCJoNp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">** Add the image_count column to the dataframe**"
      ]
    },
    {
      "metadata": {
        "id": "kgK-fR7Gig4B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3247
        },
        "outputId": "8a6b4f5a-0e6a-4410-c3a7-7851eb7a3715"
      },
      "cell_type": "code",
      "source": [
        "df['images_count'] = images\n",
        "\n",
        "print(df)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                             full_text  \\\n",
            "0    RT @IIITDelhi: Applications open for MTech (CB...   \n",
            "1    RT @IIITDelhi: We are delighted to share that ...   \n",
            "2    RT @Harvard: Professor Jelani Nelson founded A...   \n",
            "3    RT @emnlp2019: For anyone interested in submit...   \n",
            "4    RT @multimediaeval: Announcing the 2019 MediaE...   \n",
            "5    Many Congratulations to @midasIIITD student, S...   \n",
            "6    @midasIIITD thanks all students who have appea...   \n",
            "7    @himanchalchandr Meanwhile, complete CV/NLP ta...   \n",
            "8    @sayangdipto123 Submit as per the guideline ag...   \n",
            "9    We request all students whose interview are sc...   \n",
            "10   Other queries: \"none of the Tweeter Apis give ...   \n",
            "11   Other queries: \"do we have to make two differe...   \n",
            "12   Other queries: \"If using Twitter api, it does ...   \n",
            "13   Response to some queries asked by students on ...   \n",
            "14   RT @kdnuggets: Top 8 #Free Must-Read #Books on...   \n",
            "15   @nupur_baghel @PennDATS Congratulation @nupur_...   \n",
            "16   We have emailed the task details to all candid...   \n",
            "17   RT @rfpvjr: Our NAACL paper on polarization in...   \n",
            "18   RT @kdnuggets: Effective Transfer Learning For...   \n",
            "19   RT @stanfordnlp: What’s new in @Stanford CS224...   \n",
            "20   RT @DeepMindAI: Today we're releasing a large-...   \n",
            "21   RT @ylecun: Congratulations Jitendra Malik !\\n...   \n",
            "22   RT @IIITDelhi: Another chance to take admissio...   \n",
            "23   Dear @midasIIITD internship candidates who hav...   \n",
            "24   Looking forward to your paper submission to @I...   \n",
            "25   RT @ngrams: Reproducibility in multimedia rese...   \n",
            "26   Online application for https://t.co/DJFDrQsHZP...   \n",
            "27   RT @ACMMM19: A final reminder of the Reproduci...   \n",
            "28   RT @isarth23: Thanks for the support and help ...   \n",
            "29   Since SemEval-2019 will be held June 6-7, 2019...   \n",
            "..                                                 ...   \n",
            "169  RT @RatnRajiv: Fortunate to have appreciation ...   \n",
            "170  RT @Harvard: The study provides potential futu...   \n",
            "171  RT @UniofOxford: In the final episode of serie...   \n",
            "172  @ACMMM19: Call for Workshop Proposals \\n\\nDead...   \n",
            "173  @ACMMM19: Call for Grand Challenge Proposals\\n...   \n",
            "174  Since 1993, ACM Multimedia has been bringing t...   \n",
            "175  @ACMMM19 is structured around four themes (alp...   \n",
            "176  ACM Multimedia 2019: First Call for Papers/Cal...   \n",
            "177  RT @ACMMM19: Happy new year! 2019 started with...   \n",
            "178  You may follow @midasIIITD updates through fol...   \n",
            "179  Today, @midasIIITD completed one year at @IIIT...   \n",
            "180  RT @debanjanbhucs: Happy anniversary to @midas...   \n",
            "181  Two Research Assistant (RA) positions are stil...   \n",
            "182  Positions for two Research Assistants (RA) are...   \n",
            "183  RT @TCoolsIT: I wrote a blogpost on CoreNLP hi...   \n",
            "184  @RatnRajiv delivered a research talk at the Ce...   \n",
            "185  @midasIIITD look forward to work together. htt...   \n",
            "186  @the_dhumketu @punnibhai @Hitkul_, FYI. https:...   \n",
            "187  RT @iiit_hyderabad: KCIS Distinguished Lecture...   \n",
            "188  RT @StanfordAILab: Our latest blog post is out...   \n",
            "189  RT @medialab: In two new papers, @PratikShahPh...   \n",
            "190  RT @RatnRajiv: Wonderful get together with our...   \n",
            "191  @ACMMM19 is the premier international conferen...   \n",
            "192  Feel free to contact us if you have any query ...   \n",
            "193  @NilayShri @RatnRajiv @ACMMM19 Looking forward...   \n",
            "194  @midasIIITD head, Dr. @RatnRajiv appointed as ...   \n",
            "195  @ACMMM19 @the_dhumketu fill the volunteer form...   \n",
            "196  RT @ACMMM19: New in 2019! Volunteer to serve a...   \n",
            "197  RT @IIITDelhi: Congratulations! authors Yaman ...   \n",
            "198  Best wishes to @midasIIITD students @_himanshu...   \n",
            "\n",
            "                         created_at  favorite_count  retweet_count  \\\n",
            "0    Wed Apr 10 04:51:26 +0000 2019               0              1   \n",
            "1    Tue Apr 09 16:45:07 +0000 2019               0             14   \n",
            "2    Tue Apr 09 05:04:27 +0000 2019               0             35   \n",
            "3    Tue Apr 09 05:04:11 +0000 2019               0             17   \n",
            "4    Mon Apr 08 19:38:09 +0000 2019               0             15   \n",
            "5    Mon Apr 08 07:08:12 +0000 2019              18              2   \n",
            "6    Mon Apr 08 03:27:42 +0000 2019               5              0   \n",
            "7    Sun Apr 07 14:17:29 +0000 2019               0              0   \n",
            "8    Sun Apr 07 14:17:09 +0000 2019               0              0   \n",
            "9    Sun Apr 07 11:43:24 +0000 2019               1              1   \n",
            "10   Sun Apr 07 06:55:19 +0000 2019               5              2   \n",
            "11   Sun Apr 07 06:53:38 +0000 2019               4              1   \n",
            "12   Sun Apr 07 05:32:27 +0000 2019               6              1   \n",
            "13   Sun Apr 07 05:29:40 +0000 2019               7              1   \n",
            "14   Sat Apr 06 17:11:29 +0000 2019               0              2   \n",
            "15   Sat Apr 06 16:43:27 +0000 2019              18              3   \n",
            "16   Fri Apr 05 16:08:37 +0000 2019              11              1   \n",
            "17   Fri Apr 05 04:05:11 +0000 2019               0             16   \n",
            "18   Fri Apr 05 04:04:43 +0000 2019               0             11   \n",
            "19   Wed Apr 03 18:31:53 +0000 2019               0             59   \n",
            "20   Wed Apr 03 17:04:32 +0000 2019               0            872   \n",
            "21   Wed Apr 03 09:03:40 +0000 2019               0             16   \n",
            "22   Wed Apr 03 07:46:02 +0000 2019               0              4   \n",
            "23   Tue Apr 02 04:20:13 +0000 2019               8              1   \n",
            "24   Tue Apr 02 02:44:54 +0000 2019               5              1   \n",
            "25   Tue Apr 02 02:35:44 +0000 2019               0              7   \n",
            "26   Mon Apr 01 06:53:08 +0000 2019               7              2   \n",
            "27   Sun Mar 31 10:21:24 +0000 2019               0             10   \n",
            "28   Fri Mar 29 19:43:24 +0000 2019               0              2   \n",
            "29   Fri Mar 29 17:16:40 +0000 2019               9              1   \n",
            "..                              ...             ...            ...   \n",
            "169  Thu Jan 10 08:48:17 +0000 2019               0              3   \n",
            "170  Wed Jan 09 02:05:06 +0000 2019               0              9   \n",
            "171  Mon Jan 07 09:05:12 +0000 2019               0             16   \n",
            "172  Sun Jan 06 12:54:06 +0000 2019               1              0   \n",
            "173  Sun Jan 06 04:29:25 +0000 2019               1              0   \n",
            "174  Sun Jan 06 04:09:54 +0000 2019               5              1   \n",
            "175  Sun Jan 06 04:07:29 +0000 2019               1              0   \n",
            "176  Sun Jan 06 04:05:38 +0000 2019               4              2   \n",
            "177  Thu Jan 03 06:14:05 +0000 2019               0              2   \n",
            "178  Tue Jan 01 17:21:49 +0000 2019               4              2   \n",
            "179  Tue Jan 01 17:17:56 +0000 2019              14              5   \n",
            "180  Tue Jan 01 16:14:24 +0000 2019               0              4   \n",
            "181  Thu Dec 27 10:28:31 +0000 2018               2              1   \n",
            "182  Thu Dec 27 09:59:06 +0000 2018               5              3   \n",
            "183  Thu Dec 27 02:33:58 +0000 2018               0             12   \n",
            "184  Wed Dec 26 09:41:04 +0000 2018              14              2   \n",
            "185  Tue Dec 25 04:15:15 +0000 2018               1              0   \n",
            "186  Tue Dec 25 03:00:21 +0000 2018               2              1   \n",
            "187  Mon Dec 24 07:39:21 +0000 2018               0             11   \n",
            "188  Fri Dec 21 03:50:39 +0000 2018               0             32   \n",
            "189  Thu Dec 20 18:05:35 +0000 2018               0              8   \n",
            "190  Thu Dec 20 06:14:52 +0000 2018               0              3   \n",
            "191  Wed Dec 19 02:16:31 +0000 2018               2              3   \n",
            "192  Tue Dec 18 14:42:27 +0000 2018               3              2   \n",
            "193  Tue Dec 18 14:27:12 +0000 2018               3              0   \n",
            "194  Tue Dec 18 14:15:32 +0000 2018              12              5   \n",
            "195  Tue Dec 18 12:34:10 +0000 2018               1              0   \n",
            "196  Tue Dec 18 12:33:42 +0000 2018               0              7   \n",
            "197  Tue Dec 18 04:38:35 +0000 2018               0              3   \n",
            "198  Mon Dec 17 16:57:00 +0000 2018              14              4   \n",
            "\n",
            "     images_count  \n",
            "0             NaN  \n",
            "1             NaN  \n",
            "2             NaN  \n",
            "3             NaN  \n",
            "4             NaN  \n",
            "5             1.0  \n",
            "6             1.0  \n",
            "7             NaN  \n",
            "8             NaN  \n",
            "9             NaN  \n",
            "10            NaN  \n",
            "11            NaN  \n",
            "12            NaN  \n",
            "13            NaN  \n",
            "14            NaN  \n",
            "15            1.0  \n",
            "16            NaN  \n",
            "17            NaN  \n",
            "18            1.0  \n",
            "19            NaN  \n",
            "20            NaN  \n",
            "21            NaN  \n",
            "22            NaN  \n",
            "23            NaN  \n",
            "24            NaN  \n",
            "25            NaN  \n",
            "26            NaN  \n",
            "27            NaN  \n",
            "28            NaN  \n",
            "29            NaN  \n",
            "..            ...  \n",
            "169           NaN  \n",
            "170           NaN  \n",
            "171           NaN  \n",
            "172           NaN  \n",
            "173           NaN  \n",
            "174           NaN  \n",
            "175           NaN  \n",
            "176           NaN  \n",
            "177           NaN  \n",
            "178           NaN  \n",
            "179           1.0  \n",
            "180           NaN  \n",
            "181           NaN  \n",
            "182           NaN  \n",
            "183           NaN  \n",
            "184           1.0  \n",
            "185           NaN  \n",
            "186           NaN  \n",
            "187           NaN  \n",
            "188           NaN  \n",
            "189           NaN  \n",
            "190           NaN  \n",
            "191           1.0  \n",
            "192           NaN  \n",
            "193           NaN  \n",
            "194           NaN  \n",
            "195           NaN  \n",
            "196           NaN  \n",
            "197           NaN  \n",
            "198           1.0  \n",
            "\n",
            "[199 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}